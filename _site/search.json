[
  
    {
      "title": "Awesome-AIGC-Detection",
      "categories": "深度学习, 论文整理",
      "content": "本文取自笔者参与合作的github repo文档，并分工于大部分根据论文列表查阅搜集相关材料的工作。该repo的仓库地址为🔗；该repo基于一已有项目戳这里构建（给前辈们撒花致谢🎉），并且对截止2025.7.2之前的AIGC检测相关论文进行整理。照片本是记录生活的介质，如今却因AI技术面临”信任危机”——AIGC的飞速发展，让”PS都弱爆了”的时代真正来临。当眼见不再为实，如何鉴别AI生成的”照骗”已成为AI安全领域的头号挑战之一。本清单系统整理AIGC图像检测相关研究，希望对大家有所帮助。📚Datasets            Year      Dataset      Number of Real      Number of Fake      Source of Real Image      Generation Method of Fake Image                  2020      CNNSpot      362,000      362,000      LSUN, ImageNet, CelebA, COCO…      ProGAN, StyleGAN, BigGAN, CRN, SITD…              2023      GenImage      1,331,167      1,350,000      ImagNet      SDMs, Midjourney, BigGAN              2023      Fake2M      -      2,300,000      CC3M      SD-V1.5, IF, StyleGAN3              2023      DMimage      200,000      200,000      COOC, LSUN      LDM              2023      DiffusionDB      3,300,000      16,000,000      DiscordChatExporter      SD              2024      WildFake      2,557,278      1,013,446      ImagNet, Laion, Wukong, COO…      BigGAN, StyleGAN, StarGAN, Midjourney, DALLE…      📝Papers2️⃣0️⃣2️⃣5️⃣   Few-Shot Learner Generalizes Across AI-Generated Image Detection  [Paper] [Code]   Forensic Self-Descriptions Are All You Need for Zero-Shot Detection, Open-Set Source Attribution, and Clustering of AI-generated Images   [Paper]   Beyond Generation: A Diffusion-based Low-level Feature Extractor for Detecting AI-generated Images    [Paper]   Secret Lies in Color: Enhancing AI-Generated Images Detection with Color Distribution Analysis     [Paper]   Towards Universal AI-Generated Image Detection by Variational Information Bottleneck Network    [Paper]   Where’s the Liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content    [Paper]   FIRE: Robust Detection of Diffusion-Generated Images via Frequency-Guided Reconstruction Error   [Paper]   Forensic Self-Descriptions Are All You Need for Zero-Shot Detection, Open-Set Source Attribution, and Clustering of AI-generated Images    [Paper]   Orthogonal Subspace Decomposition for Generalizable AI-Generated Image Detection    [Paper] [code]   D3 Scaling Up Deepfake Detection by Learning   [Paper] [Code]   SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model     [Paper] [Code]   A Bias-Free Training Paradigm for More General AI-generated Image Detection    [Paper] [Code]   Any-Resolution AI-Generated Image Detection by Spectral Learning   [Paper] [Code]   TextureCrop: Enhancing Synthetic Image Detection through Texture-based Cropping    [Paper] [Code]   HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator   [Paper] [Datasets] [Code]   Improving Synthetic Image Detection Towards Generalization: An Image Transformation Perspective     [Paper] [Code]   SFLD: Reducing the content bias for AI-generated Image Detection   [Paper]   A SANITY CHECK FOR AI-GENERATED IMAGE DETECTION     [Paper] [Code]   FAKESHIELD: EXPLAINABLE IMAGE FORGERY DETECTION AND LOCALIZATION VIA MULTI-MODAL LARGE LANGUAGE MODELS   [Paper] [Code]   Exploring the Collaborative Advantage of Low-level Information on Generalizable AI-Generated Image Detection   [Paper]   HYPERDET: GENERALIZABLE DETECTION OF SYNTHESIZED IMAGES BY GENERATING AND MERGING A MIXTURE OF HYPER LORAS    [Paper]   Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image Detection with Forgery Amplification Approach   [Paper] [Code]   AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors  [Paper] [Code]   Can GPT tell us why these images are synthesized? Empowering Multimodal Large Language Models for Forensics   [Paper]   Exploring Modality Disruption in Multimodal Fake News Detection    [Paper]2️⃣0️⃣2️⃣4️⃣   FakeInversion: Learning to Detect Images from Unseen Text-to-Image Models by Inverting Stable Diffusion  [Paper] [Code]   Forgery-aware Adaptive Transformer for Generalizable Synthetic   [Paper] [Code]   Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection  [Paper] [Code]   LaRE2: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection  [Paper] [Code]   AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error   [Paper] [Code]   MiRAGeNews: Multimodal Realistic AI-Generated News Detection    [Paper] [Code]   Zero-Shot Detection of AI-Generated Images   [Paper] [Code]   Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection   [Paper] [Code]   Contrasting Deepfakes Diffusion via Contrastive Learning and Global-Local Similarities    [Paper] [Code]   DRCT: Diffusion Reconstruction Contrastive Training towards Universal Detection of Diffusion Generated Images [Paper] [Code]   Exposing the Fake: Effective Diffusion-Generated Images Detection    [Paper] [Code]   How to Trace Latent Generative Model Generated Images without Artificial Watermark?    [Paper] [Code]   CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection    [Paper] [Code]   Harnessing the Power of Large Vision Language Models for Synthetic Image Detection    [Paper] [Code]   DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models    [Paper] [Code]   Raising the Bar of AI-generated Image Detection with CLIP    [Paper] [Code]   Faster Than Lies: Real-time Deepfake Detection using Binary Neural Networks    [Paper] [Code]   Breaking Semantic Artifacts for Generalized AI-generated Image Detection    [Paper] [Code]   Mixture of Low-rank Experts for Transferable AI-Generated Image Detection     [Paper] [Code]   Guided and Fused: Efficient Frozen CLIP-ViT with Feature Guidance and Multi-Stage Feature Fusion for Generalizable Deepfake Detection     [Paper]   A Single Simple Patch is All You Need for AI-generated Image Detection    [Paper] [Code]   RIGID: A Training-Free and Model-Agnostic Framework for Robust AI-Generated Image Detection   [Paper]   Improving Interpretability and Robustness for the Detection of AI-Generated Images   [Paper]   Continuous fake media detection: adapting deepfake detectors to new generative techniques   [Paper]   Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?    [Paper]   Let Real Images be as a Judger, Spotting Fake Images Synthesized with Generative Models    [Paper]   Detecting Image Attribution for Text-to-Image Diffusion Models in RGB and Beyond   [Paper] [Code] [Code]   MLEP: Multi-granularity Local Entropy Patterns for Generalized AI-generated Image Detection    [Paper]   FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant    [Paper] [Code]2️⃣0️⃣2️⃣3️⃣   Towards Universal Fake Image Detectors that Generalize Across Generative Models  [Paper] [Code]   Learning on Gradients: Generalized Artifacts Representation for GAN-Generated Images Detection    [Paper] [Code]   DIRE for Diffusion-Generated Image Detection   [Paper] [Code]   Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images    [Paper] [Code]   On The Detection of Synthetic Images Generated by Diffusion Models   [Paper] [Code]   DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models    [Paper]   Synthbuster: Towards Detection of Diffusion Model Generated Images   [Paper]   Online Detection of AI-Generated Images   [Paper]   Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality    [Paper]   PatchCraft: Exploring Texture Patch for Efficient AI-generated Image Detection   [Paper]   Generalizable Synthetic Image Detection via Language-guided Contrastive Learning   [Paper] [Code]   Raising the Bar of AI-generated Image Detection with CLIP    [Paper] [Code]   GenDet: Towards Good Generalizations for AI-Generated Image Detection    [Paper]   AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors   [Paper]   Generalizable Synthetic Image Detection via Language-guided Contrastive Learning   [Paper] [Code]   PatchCraft: Exploring Texture Patch for Efficient AI-generated Image Detection    [Paper] [Code]2️⃣0️⃣2️⃣2️⃣   Detecting Generated Images by Real Images  [Paper] [Code]   FingerprintNet: Synthesized Fingerprints for Generated Image Detection    [Paper]2️⃣0️⃣2️⃣1️⃣   Are GAN generated images easy to detect? A critical analysis of the state-of-the-art  [Paper]2️⃣0️⃣2️⃣0️⃣   CNN-generated images are surprisingly easy to spot… for now   [Paper] [Code]   Global Texture Enhancement for Fake Face Detection In the Wild   [Paper] [Code]   Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are Failing to Reproduce Spectral Distributions   [Paper] [Code]   What makes fake images detectable? Understanding properties that generalize   [Paper] [Code]   FingerprintNet: Synthesized Fingerprints for Generated Image Detection    [Paper] [Code]🏷️（代码貌似非官方）",
      "url": "/posts/Awesome_AIGC_Detection/",
      "date": "2025年08月15日"
    }
    ,
  
    {
      "title": "Jekyll博客搭建教程（上）",
      "categories": "Jekyll, 教程",
      "content": "PS：  笔者想要写这篇教程的初衷是因为放眼b站等平台，对于Jekyll框架的教程局限于相关环境配置以及如何去发布自己的博客文章，但是笔者经过将material-you主题进行魔改后才发现，目前还没有找到结构性的教程；教你如何套用以及如何上传文章的通俗简易教程比比皆是，但是如果你想要改成自己喜欢的样式的话就很难了。  笔者也是让gpt和gemini成为左膀右臂磨石子过河终于弄清了框架的运作模式，但是如果大家有一份和笔者一样的魔改的心，那么希望下面的内容可以给你减少查阅资料和寻找教程所需花费的精力。  由于笔者电脑系统为MacOS，然而为了大家方便，前期环境配置内容若有区别都会将Mac和Windows模块进行区分，以及我之前已经完成的安装模块我就不重复操作进行截屏配图了，有看不懂的请在评论区进行留言～～一、安装Ruby环境MacOS系统macOS 虽然自带 Ruby，但为了避免权限问题和便于版本管理，一般使用 Homebrew 和 rbenv。  安装Homebrew：如果没有安装过Homebrew的话在终端运行下面这个命令；    /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"        按照提示输入密码等待即可安装完成（值得注意的是输入密码的时候窗口不会有任何响应，只管输入即可）    安装并配置rbenv：rbenv是Ruby版本的管理工具接下来我们用Homebrew对rbenv进行下载    brew install rbenv        然后将rbenv添加到你的shell配置文件中（默认为～/.zshrc），这样每次打开中断的时候rbenv都会自动加载  echo 'if which rbenv &gt; /dev/null; then eval \"$(rbenv init - zsh)\"; fi' &gt;&gt; ~/.zshrcsource ~/.zshrc  安装Ruby：接着我们要使用rbenv安装一个最新且稳定的Ruby版本，并且将其设为全局默认版本，防止后期出现不必要的麻烦    rbenv install 3.3.0rbenv global 3.3.0ruby -v        ruby -v 用于验证版本是否已切换为3.3.0  Windows系统🖥️Windows不自带Ruby，所以需要手动安装确保开发环境完整。  安装 Git for Windows：          访问Git for Windows官网下载安装程序。      在安装过程中，强烈建议选择使用 Git Bash 作为默认终端，它可以提供类 Unix 环境，避免很多路径和命令问题。(eg:Win系统在cmd输入路径的时候一般使用反斜杠’',而在Jekyll的配置性文件中我们填写路径时用斜杠‘/’)安装完成后，以后所有命令都在 Git Bash 中执行。        安装 RubyInstaller：          访问RubyInstaller for Windows官网。      下载并运行Ruby+Devkit版本。这个版本包含了所有必要的开发工具，可以顺利安装 Jekyll。安装时请确保勾选 “Add Ruby executables to your PATH” 选项。安装完成后，会弹出一个命令行窗口，询问您是否安装 MSYS2 等依赖。请选择默认选项（通常是 1, 2, 3），按回车键完成安装。      二、安装Jekyll和Bundler🎉该步骤MacOS与Windows方法相同故不再分模块啦！  安装Bundler：Bundler 是一个 Ruby 的依赖管理工具，后期本地预览调试的时候经常用到。    gem install bundler        安装Jekyll：Jekyll 本身也是一个 Ruby Gem    gem install jekyll        创建和初始化博客📌    首先选择目录，你要将你的博客文件创建在哪里，然后通过cd命令行进入你要创建的所在目录。创建新博客：运行以下命令，Jekyll 会创建一个名为 my-jekyll-blog 的新文件夹。    jekyll new my-jekyll-blog          可将my-jekyll-blog替换成你想要的文件夹名称，生成的文件结构如下：分几种情况：  部署在GitHub Pages上：  你先将你的“用户名.github.io“仓库克隆下来，当然里面是空白的或者是仅有README；然后你在仓库文件夹所在的目录生成你的my-jekyll-blog文件夹，然后将文件夹里的内容全部转移到仓库说在文件夹里。  之后上传到服务器：  创建好你的my-jekyll-blog文件夹后在里面编辑即可现在进入目录并安装依赖,这会根据 Gemfile 安装 Jekyll 及其主题所需的所有 Gem 包。cd my-jekyll-blogbundle install可见有警告信息 [DEPRECATED] Platform :mingw, :x64_mingw, :mswin is deprecated. Please use platform :windows instead.  [DEPRECATED]表示“已弃用”，这意味着这个功能或写法在未来版本的 Bundler 中可能会被移除或不再支持；  Platform :mingw, :x64_mingw, :mswin is deprecated.: 明确指出，在 Gemfile 中用来指定 Windows 平台的写法（platform :mingw, platform :x64_mingw, platform :mswin）已经过时了。解决方案：打开Gemfile文件（在Jekyll博客项目的根目录）找到并修改platform配置，将gem “tzinfo-data”, platforms: [:mingw, :x64_mingw, :mswin]改为gem “tzinfo-data”, platforms: :windows👆这条针对windows系统，只有从Windows环境的jekyll中复制过来才会出现这个警告⚠️，然而你不解决也不会对你的网站部署造成任何影响❗️为了避免中文字乱码，一定一定要打开_config.yml文件添加配置：encoding: utf-8并且确保你的编辑器是以UTF-8格式保存所有文件。  本地预览：确保现在在博客文件夹这个目录里，输入以下命令启动本地服务器    bundle exec jekyll serve        然后在浏览器中访问http://localhost:4000，即可看到默认博客页面。  如上图是jekyll默认的主题页面，至此你的博客网站已经搭建成功，内容的上传以及样式的修改且听下回分解",
      "url": "/posts/Jekyll%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B-%E4%B8%8A/",
      "date": "2025年08月12日"
    }
    ,
  
    {
      "title": "Pytorch笔记📓第一弹",
      "categories": "深度学习, Pytorch",
      "content": "Pytorch的核心——张量在深度学习中，所有的数据和模型参数都用张量表示。张量（Tensor）可以看成是：  0 维：标量（scalar），比如 3  1 维：向量（vector），比如 [1, 2, 3]  2 维：矩阵（matrix）  3 维及以上：高维数组，比如一批图片 (batch_size, height, width, channels)📌 在 PyTorch 里：  torch.tensor() 创建张量  .shape 查看维度  .dtype 数据类型  .to(device) 把张量放到 CPU/GPUGPU加速如果有GPU，PyTorch 会自动帮你用 CUDA 加速device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")x = torch.randn(3, 4).to(device)自动求导PyTorch 可以自动计算梯度，这就是深度学习训练的基础。  创建张量时加 requires_grad=True，PyTorch 会记录计算图。  调用 .backward() 自动反向传播，计算梯度。  梯度存放在 .grad 里。第一个Pytorch文件#myfirstPytorchimport torchprint(torch.__version__)  x=torch.tensor([[1,2],[3,4]],dtype=torch.float32)y=torch.ones((2,2))print(\"x:\\n\",x)print(\"y:\\n\",y)z=x+yprint(\"z:\\n\",z)device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")x=x.to(device)y=y.to(device)x=x.to(device)print(\"设备:\",device)  a=torch.randn(3,4,requires_grad=True)b=torch.randn(3,4)c=a*bout=c.sum()out.backward()print(\"a的梯度:\\n\",a.grad)#运行结果2.6.0x: tensor([[1., 2.],        [3., 4.]])y: tensor([[1., 1.],        [1., 1.]])x: tensor([[2., 3.],        [4., 5.]])设备: cpua的梯度: tensor([[ 0.4779,  1.5350,  0.0037,  1.2580],        [-0.5812,  1.6379,  0.7242,  1.2655],        [-0.6917, -0.8033, -1.8678, -1.1883]])进阶任务 🖊️  创建一个 (2, 3, 4) 的随机张量 t（float 类型）  对它做：          加法（+ 1）      减法（- 0.5）      矩阵乘法（用 .matmul() 或 @）*        把它移到 GPU（如果没有 GPU 就依然用 CPU），并打印：          .shape      .dtype      import torcht=torch.randn(2,3,4,dtype=torch.float32)x=torch.ones((2,3,4))print(\"t:\\n\",t)a=t+xb=t-x*0.5  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")# t=t.to(device)# x=x.to(device)# a=a.to(device)# b=b.to(device)t, x, a, b=[tensor.to(device) for tensor in [t, x, a, b]]print(\"设备:\\n\",device)print(\"a:\\n\",a)print(\"b:\\n\",b)  print(\".shape:\\n\",t.shape)print(\".dtype:\\n\",t.dtype)#运行结果t: tensor([[[-0.5283,  1.2119,  1.6814,  0.6434],         [-0.3742, -0.8421, -1.6161,  0.2300],         [-1.2224,  0.4019, -1.4070,  0.4027]],        [[ 0.0407, -0.5536, -0.7496,  1.7721],         [ 1.4842,  0.2181, -0.0732, -1.1741],         [ 0.4077, -0.7331, -2.2628,  0.8560]]])设备: cpua: tensor([[[ 0.4717,  2.2119,  2.6814,  1.6434],         [ 0.6258,  0.1579, -0.6161,  1.2300],         [-0.2224,  1.4019, -0.4070,  1.4027]],        [[ 1.0407,  0.4464,  0.2504,  2.7721],         [ 2.4842,  1.2181,  0.9268, -0.1741],         [ 1.4077,  0.2669, -1.2628,  1.8560]]])b: tensor([[[-1.0283,  0.7119,  1.1814,  0.1434],         [-0.8742, -1.3421, -2.1161, -0.2700],         [-1.7224, -0.0981, -1.9070, -0.0973]],        [[-0.4593, -1.0536, -1.2496,  1.2721],         [ 0.9842, -0.2819, -0.5732, -1.6741],         [-0.0923, -1.2331, -2.7628,  0.3560]]]).shape: torch.Size([2, 3, 4]).dtype: torch.float32   矩阵乘法的实现——用 .matmul() 或 @因为张量t相当于2个3* 4的矩阵，不能直接和自己相乘，所以需要再随机生成一个（2，4，k）的矩阵//令k=6import torcht=torch.randn(2,3,4,dtype=torch.float32)s=torch.randn(2,4,6,dtype=torch.float32)  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")t,s=[tensor.to(device) for tensor in [t,s]]print(\"设备:\\n\",device)  out=torch.matmul(t,s)#或者out=t@sprint(\"out:\\n\",out)print(\".shape:\\n\",out.shape)print(\".dtype:\\n\",out.dtype)#运行结果设备: cpuout: tensor([[[-0.6649, -0.7129,  1.1296, -1.8482,  1.4739,  0.7178],         [-1.5440,  4.1733, -3.7995, -1.6282, -3.5405, -0.3269],         [ 2.4297, -4.6300,  0.9426,  2.7147,  1.8590,  4.4622]],        [[ 1.0501,  0.4767,  1.8069, -3.8165,  2.4047, -1.2312],         [-1.2006,  0.1548, -0.6207,  1.4950, -1.4686,  1.2318],         [-1.5264,  0.0297, -0.2412, -1.4789,  1.6743, -1.2765]]]).shape: torch.Size([2, 3, 6]).dtype: torch.float32 ",
      "url": "/posts/pytorch1/",
      "date": "2025年08月10日"
    }
    ,
  
    {
      "title": "LLM简介",
      "categories": "深度学习",
      "content": "LLM的训练历程非神经网络时代的完全监督学习（特征工程）  利用特定的规则，统计学的模型对特征进行匹配和利用，进而完成特定的NLP任务——&gt;常见方法：贝叶斯隐马尔可夫模型等基于神经网络的完全监督学习 （架构工程）  不用手动设置特征和规则，节省了人力资源，但需要人工设计合适的神经网络架构对数据集进行训练——&gt;常见方法：CNN、RNN、Transformer等预训练+精调范式（目标工程）  现在数据集上进行训练，然后里用预训练好的模型在下游任务的特定数据集上进行fine-tuning，使模型更适应下游任务预训练+提示——预测范式（prompt工程）  将下游任务的建模方式重新定义，通过合适的prompt来实现直接在预训练模型上解决下游任务，这种模型需要极少的（甚至为零）下游任务数据，使小样本、零样本学习成为可能LLM主要类别介绍Transformer分类：  BERT：只使用了transformer编码器encoder部分而完全舍弃解码器decoder          预训练任务：                  Masked LM：随机掩盖掉一些词然后联系上下文进行预测该词（完形填空）          Next Sentence Prediction：判断第二个句子是不是第一个句子的下文                      GPT：采用了transformer经典架构的解码器decoder但是未完全照搬（去除了中间段的muti- head attention，并且相比经典架构6个decoder block，GPT采用了12个）          预训练任务：                  无监督预训练（语言建模）：模型根据前面的文本来预测文本的下一个词          训练目标：最大似然估计（最大化正确预测下一个词的概率                      T5：          相当于将所有任务都转化为文本转化任务，进行任务统一的结构      主题架构还是Transformer，但采用简化版的规范化层；使用一种简单版的相对位置编码， 在同一层内不同注意头的位置编码都是独立学习的      预训练任务：                  自监督预训练：Masked LM（同BERT）          多任务预训练：利用不同任务的标注数据进行有监督的多任务的预训练                    ",
      "url": "/posts/LLM%E7%AE%80%E4%BB%8B/",
      "date": "2025年07月20日"
    }
    ,
  
    {
      "title": "MLLM的AIGC检测",
      "categories": "深度学习, 论文笔记",
      "content": "Can GPT Tell Us Why These Images Are Synthesized? Empowering MLLMs for Forensics点击下载原文pdf❤️✏️摘要  background：MLLMs不擅长通过发现图像的伪造细节对抗鉴别AIGC的生成图像  methods：从语义层面分析伪造线索的框架（评估图像真实性、定位篡改区域、提供证并追踪生成方法）          research：GPT-4v在Autosplice数据集上的准确率为92.1%，在LaMa数据集上为86.3%（性能接近当前最先进的AIGC检测方法）      discussion：MLLMs在该任务重的局限性以及未来的改进方向      💡引言在影像生成消极影响猖獗的情况下，目前对于deepfake的辨别方法大多依赖于小规模的机器学习模型，尤其是CNN和光流分析。\t光流是计算机视觉中用于估计图像序列中像素运动的技术，通过分析连续帧（如视频）中物体的运动模式，得到每个像素的运动矢量场。其核心假设是亮度恒常性（同一像素在相邻帧中的亮度不变）。典型应用场景：    运动目标检测与跟踪、视频稳像（Video Stabilization）、自动驾驶（场景流估计）、动作识别（人体运动分析）当下虽有MLLMs的兴起促成视觉与语言的合并与统一，然而它们对于生成图像的识别性能仍然有限。该研究通过激发MLLMs对于伪造识别的文本分析能力填补这个研究空白，（传统的深度伪造识别技术仅停留于例如像素不一致性和频率分析等固有方法）并如下图分两个步骤：  1.识别图像的真伪与否  2.分析找出对应的伪造异常点例如该图中：  异常锐化的不自然的边缘  不和谐的亮面高光  两个单面煎的蛋黄（通常认为是一个，忽略双黄蛋的情况）研究贡献分析：  MLLMs在预训练过程中就已经获得的语义认知可以助于其辨别自然和人工智能生成的图片，并且不像传统的机器学习辨别方法，大模型可以关于他们认定的选项（自然or生成）提供人类可理解的解释。  在激发大模型伪造分析能力时基于五个基本原则设计了提示，并采用联系上下文（ICL）的策略，使多模态大模型展现了识别并描述伪造特征并追踪伪造方法的能力  该研究方法能充分利用大模型的多任务处理能力，在识别生成图像的准确率达到92.1%📁相关工作合成图像检测🔍该研究主要采用了三种方法：  1.空间特征学习✨：从RGB输入中提取空间特征，一些方法使用全局特征，另一些则强调低级特征和局部图像块来提高检测效果。  2.频域特征学习：利用频率域分析识别伪造图像中的伪影，使用如频谱幅度和二维傅里叶变换（ 2D-FFT）等特征。  3.特征融合方法：将多种特征（如RGB和YCbCr颜色空间、频率与空间特征）融合，以增强对AI合成图像的检测能力（兼具1和2）多模态大模型将大模型文本与图像的对其统一得到的多模态大模型可用于医疗诊断应用、对视频的理解归纳和对图像的编辑，前景光明。📖方法论结构概述目标包含两个议题：  利用大模型的文本理解能力和已有知识去分析和判断图像  进一步辅助定位出特定伪造区域并解释伪造方式解决方式：  ～ 一种直观的方法：提示和微调一个大规模的多模态大模型来输出判断结果以及分析Shortcomings：多任务训练会导致网络操作的难度大大增加，且各进程会相互影响和干扰。🔴另一可行的方法：Stage1:  通过提供输入伪造图片进行训练，然后执行二分类法是多模态大模型根据已有知识判断已知图片的真伪Stage2:  1.定位到伪造的区域  2.描述伪造的特征  3.提供判断伪造的理由  4.追踪伪造对应使用的方法该方法可行性：  该方法结构与人类的认知过程一致性,  先有个粗糙的判断然后挖掘细节去证实判断  众多成功的案例:  FakeShield、 ForgeryGPT、 PorFact-NET  过长的提示词导致模型幻觉加重,  “先判断后分析”的模式可以尽可能大的避免并不破坏MLLMs的性能*实验最终表明 该方法对于对抗生成和扩散生成的图像都具有较为令人满意的结果文本提示📝提示词在引导模型判断图片真伪时具有关键作用，在现有的研究文献中所示，过于简单的提示词通常是无效的。  信息缺乏or安全隐患 –&gt; 不准确的答案或是拒绝回答为应对挑战，要寻找平衡使既不至于文本过于简单导致无效，又不至于文本过于复杂导致幻觉5️⃣ Prompts五个原则：\t//灵感来自于LangGPT的设计 并参考OpenAI官方文档  Profile：“You are an AI visual assistant”  Goal: “ help humans analyze some tampered images”  Constraint: “Must return with yes or no only”      Workflow: “you will receive one image, your job is to determine if the image is  tampered or not.”    Style： 语言风格//上图共有5个指示示例，其中经过实践，Prompt#4达到最优的平衡Stage2中，引导大模型模仿人类鉴定的工作流程，同时将拒绝和幻觉的可能性降到最低\t//ICL的使用使准确率上升12%，被拒绝率显著下降Stage1:伪造检测将图片真伪判断视作二元分类任务，并且在提示中加入了真实样本和伪造样本得到：  加入示例使拒答率大大降低  简洁的提示比复杂的描述更加有用Stage2:伪造分析LLM已经具有检测伪造数据的能力（in stage1）但是要精确描述其中伪造篡改的区域还具有两个缺陷：  难以生成细致的描述性信息（对人类语言的适应度不佳  无法准确阐述图像的语义内容（不擅长对图像的读取与理解 *然而MLLM具有更强的语义理解能力 能够在stage2对深度伪造图像展开细粒度分析。需要从图像中提取的关键信息：  篡改区域定位：图像中被伪造的具体位置  篡改内容解析：该位置的元素构成（具体是什么）  篡改痕迹显化：突出显示其视觉上异常和不协调的细节  生成方法判定：          明确对应的技术（eg：对抗生成or扩散模型）      伪造层级（整体or局部）      如图例1.篡改定位：  相对定位：人群的上方，建筑前面  绝对定位：图像的中心稍微偏右2.篡改解析：  篡改区域的人拿着一个旗帜，然而该旗帜是后期添加的3.痕迹显化：  色彩饱和度的过分突兀  旗帜边缘被过分锐化，既不自然  旗帜的阴影与图像真实部分不匹配4.技术判定：  扩散模型  local forgery：在原图基础上进行篡改🧪实验操作实验设置数据集：真实图像  1000真实通用图像（来自 Caltech-101）  1000真实人脸图像（来自Caltech-WebFaces）生成图像  4000全局伪造图像（来自Stable Diffusion和StyleGAN）  4000局部伪造图像（来自AutoSplice和LaMa）  各1000生成人脸图像（来自 AutoSplice 和HiSD）//这些数据集兼具GAN和Diffusion两种伪造方式，保障大模型的综合能力类比前沿方法      FreDect：利用频域分析        GramNet：增强全局纹理表现        CNNSpot：CNN生成图像会具有伪影痕迹，根据这一点进行识别  鉴别指标在二分类任务中：      stage1的判定图像真伪        stage2的判别技术是对抗生成or扩散  ——&gt;概率评分方法多次查询大模型取平均分数（No=0&amp;Yes=1）advantage：  LLM生成token具一定概率，并使用top-k策略来选择输出，利于评估模型对同一张图片判断结果的一致性和多样性  使用数值决策分数在评估性能时就不局限于简单的accuracy，可以有更全面的ROC和AUC，提供了更可靠的评估          AUC允许与现有的程序检测方法总结比较，促进大模型功能在取证任务重更广泛的评估      对ROC和AUC的学习1. ROC（Receiver Operating Characteristic）ROC曲线是一种用于表示分类模型性能的图形工具。它通过将[真阳性率]和[假阳性率]作为横纵坐标来描绘分类器在不同阈值下的性能。  真阳性率 (True Positive Rate, TPR)真阳性率（True Positive Rate，TPR）通常也被称为敏感性（Sensitivity）或召回率（Recall）。它是指分类器正确识别正例的能力。真阳性率可以理解为所有阳性群体中被检测出来的比率(1-漏诊率)，因此TPR越接近1越好。它的计算公式如下：其中，TP（True Positive）表示正确识别的正例数量，FN（False Negative）表示错误地将正例识别为负例的数量。  假阳性率 (False Positive Rate, FPR)假阳性率（False Positive Rate，FPR）是指在所有实际为负例的样本中，模型错误地预测为正例的样本比例。假阳性率可以理解为所有阴性群体中被检测出来阳性的比率(误诊率)，因此FPR越接近0越好。它的计算公式如下：其中，FP（False Positive）表示错误地将负例识别为正例的数量，TN（True Negative）表示正确识别的负例数量。FPR的值等于1-特异性特异性（Specificity）是指在所有实际为负例的样本中，模型正确地预测为负例的样本比例，其衡量的是模型对负例样本的判断能力。假如一个模型的特异性很高，则该模型在预测负例时的准确率很高，也就是说，该模型较少将负例预测为正例，从而使得假阳性率较低。因此，假阳性率和特异性都是用来衡量模型在负例样本上的性能，它们之间是负相关的，即假阳性率越低，特异性越高，反之亦然。2. AUC（Area Under the Curve）AUC（ROC曲线下面积）是ROC曲线下的面积，用于衡量分类器性能。AUC值越接近1，表示分类器性能越好；反之，AUC值越接近0，表示分类器性能越差。在实际应用中，我们常常通过计算AUC值来评估分类器的性能。理论上，完美的分类器的AUC值为1，而随机分类器的AUC值为0.5。这是因为完美的分类器将所有的正例和负例完全正确地分类，而随机分类器将正例和负例的分类结果随机分布在ROC曲线上。综上，ROC曲线和AUC值是用于评估二分类模型性能的两个重要指标。通过ROC曲线，我们可以直观地了解分类器在不同阈值下的性能；而通过AUC值，我们可以对分类器的整体性能进行量化评估。roc_auc_score和roc_curve是sklearn.metrics库中的两个函数，用于评估二分类模型的性能。ROC曲线和AUC值是衡量分类器性能的两个重要指标，可以帮助我们了解模型在不同阈值下的性能。      ROC曲线：ROC曲线（Receiver Operating Characteristic Curve）是一种描绘分类器性能的图形工具，它显示了在不同阈值下分类器的真阳性率（True Positive Rate，TPR）和假阳性率（False Positive Rate，FPR）之间的关系。        AUC值： AUC（Area Under the Curve）值表示ROC曲线下的面积，用于衡量分类器性能。AUC值越接近1，表示分类器性能越好；反之，AUC值越接近0，表示分类器性能越差。  伪造位置指标根据上图所给出的例子，模型的输出包含了相对位置（relative position）和绝对位置（absolute position），这些对于模型的性能评估能量化定位精度，然而对于模型给出的回答在语义上还要有可读性（readability）和完整性（completeness）。这四个评估指标能够衡量大模型精确定位篡改区域的效率。实施细节  主要模型：GPT-4o  结合：两个开源LLM：          Llama3.2      DeepSeek-VL2        提示工程（prompt）：同图4&amp;所有LLM的双镜头学习技术（最少的数据N-shot learning技术：用最少的数据训练最多的模型）  在 N-shot学习领域中，每K个类别，我们标记了 n 个示例，这 N·K个总示例被我们称为支持集 S 。我们还必须对查询集 Q 进行分类，其中每个示例位于其中一个 K 类中。N-shot 学习有三个主要子领域：zero-shot learning、one-shot learning和小样本学习，每个领域都值得关注\t最有趣的子领域是Zero-shot learning，该领域的目标是不需要一张训练图像，就能够对未知类别进行分类。  没有任何数据可以利用的话怎么进行训练和学习呢？如果你对这个物体的外表、属性和功能有充足的信息的话，你是可以实现的。想一想，当你还是一个孩子的时候，是怎么理解这个世界的。在了解了火星的颜色和晚上的位置后，你可以在夜空中找到火星。或者你可以通过了解仙后座在天空中”基本上是一个畸形的’W’“这个信息中识别仙后座。根据今年NLP的趋势，Zero-shot learning 将变得更加有效。伪造鉴别性能\tLLM在区分真实图像生成图像方面达到了合理的精度水平，定量结果进一步支持了观察结果如图 ，左边为鉴别扩散生成的精度测量，右边为鉴别生成对抗网络的精度测量（GPT-4v准确率远大于50%，可知并不执行随机猜测）DeepSeek和Llama的准确率虽然有所下降，但还是明显优于随机猜测。      如上表的准确率比较，为验证大模型检测与传统方法之间的差异，分别分析diffusion和GAN生成数据集的结果，方法中前三个为传统方法，后三个为LLM        表中对于每个数据集将ACC最高的数据bold，根据数量可知GPT-4V（1T）的性能大大优于Llama（11B）和DeepSeek（2.8B）  conclusion：模型的大小与deepfake检测性能呈正相关        传统的鉴别方法对于真实的数据集和特定的伪造数据集（FreDect ——&gt;Style）有较强的性能。而基于LLM的鉴别方法中 deepseek泛化能力较弱，对于真实数据集ACC仅有55.2%，接近随机猜测。%%（可能的原因是模型错误的讲不寻常的特征如运动模糊或失焦视为伪造标志%%    如上表，生成方法从全局伪造转移到局部伪造时，传统的deepfake检测方法表现波动（eg：FreDect的AUC从94.2%掉到73.5%）      而LLM收到的变化影响较小    reason：局部伪造保留原始图像的大多特征，导致信号差异不那么明显容易混淆          但是LLM依赖于语义的不一致，语义的差异仍然存在与局部伪造图像，所以LLM的检测操作仍然有效        LLM在人脸图像上的表现较弱——&gt;直观原因：          人脸受到年龄肤色表情和发型等多种因素的影响，引入更加复杂的语义，该复杂性致使LLM更难区分真伪另外GPT-4V在真实和生成的人脸数据集中都展现稳定的性能。真实人脸ACC=76.7%AutoSplice和HiSD的ACC分别是79.6%和76.2%      伪造分析性能上文提到过的四个评估指标：  相对位置（relative position）  绝对位置（absolute position）  可读性（readability  完整性（completeness）          LLM虽然在高级语义理解方面表现出色，但他们难以进行细粒度的对象识别        解决方案：对特定数据集进行微调  生成方法判断的准确率如上表可知：  DeepSeek和Llama在识别由GAN生成的数据集时展现出较为强烈的偏差  DeepSeek和Llama在识别扩散生成图像时准确率显著降低          观察数据集Autosplice和Autosplice(f)的数据比较 ，可见大模型在人脸数据集上的表现更差 进一步论证了之前的结论，由于人脸的语义复杂性的增加，人脸伪造监测对llm更具挑战性      消融研究是机器学习、深度学习等领域中常用的一种实验方法，目的是通过“移除”或“修改”模型的某些组件（如层、模块、特征、训练技巧等），定量分析这些组件对模型性能的贡献。简单来说，就是通过“拆解”模型，验证每个部分是否真的有用  如上图所示，随着token数量的增加，识别ACC基本持平没有太大波动（模型的鉴别性能保持稳定），然而拒绝率在prompt#1~3逐渐下降，然而prompt #5  反而上升（虽然更长的提示能提高准确性，但是也增加了LLM运行的计算成本）范例敏感性将范例纳入prompt可以显著提高大模型学习上下文的能力\t- 实验：设计10个样本，并从池中随机抽样k=0，1，2，4个样本进行k-shot学习实验，并从AutoSplice数据集中随机选择1000个人脸图像评估平均性能，使用的是prompt #4\t- 敏感性分析如下：可得：  增加镜头数量（范例）对提高准确率的影响并不明显，但是对下降拒绝率却有很大的作用。  在没有范例时（0-shots）GPT-4V趋向于拒答与人脸有关的鉴别问题  添加示例使得LLM可以更好泛化到检测任务  对于ACC的影响，0-shots——&gt;1-shots提升了8.8%，但后期就几乎不明显，反过来说更多的样本还会增加计算成本——&gt;寻找平衡，尽可能的确保LLM受益于上下文指导但是不产生过多的成本👆提升  除了对伪造图像的检测，随着大模型生成技术的提升，其他形式的生成内容也取得重大进展，这对于DeepFake的鉴别大大增加了难度          视频中的时间一致性      音频中的光谱模式（？）  尽管LLM在图像分析中表现出很强的语义理解能力，但在视频和音频中很大程度上为得到探索。如上图结构：  将大型模型和小模型的优势进行组合——&gt;创建混合系统，利用LLM的泛化能力和专门的小模型或工具的精度来实现系力度的伪造分析。图中为一个探索性框架：数据—预处理—input—&gt;LLMLLM作为连接器和任务分配器基于预训练的知识和语义理解。LLM将特定任务分配给下游小模型或工具（detection tools） advantage：        LLM提供广泛的语义理解和任务协调      小型模型和传统鉴别工具提供高精度和效率    显著提高DeepFake检测系统的鲁棒性和可扩展性  结论该研究采用了两阶段框架（先判断后取证分析），以系统化、全面地分析可能被伪造的图像；并利用多模态大模型丰富的语义知识库的能力无需针对特定DeepFake场景进行专门训练，即可在多样数据集上表现卓越的泛化能力。",
      "url": "/posts/MLLM%E7%9A%84AIGC%E6%A3%80%E6%B5%8B/",
      "date": "2025年05月03日"
    }
    ,
  
    {
      "title": "GENIMAGE",
      "categories": "深度学习, 论文笔记",
      "content": "GENIMAGE: A MILLION-SCALE BENCHMARK FOR DETECTING AI-GENERATED IMAGE点击下载原文pdf❤️  研究背景 生成模型发展引发担忧，假图像传播影响社会稳定； eg：AI生成的五角大楼着火照片影响股市根据论文图表⬇️现有假图像检测数据集存在局限，如UADFV规模小，ForgeryNet仅关注人脸、早期通用数据集依赖GAN且数据有限。 –&gt;GenImage数据集构建的必要性数据集构建 包含超100万对真假图像，使用ImageNet所有真实图像，依1000个标签生成1350000张假图像用8种生成模型生成假图像，每个模型为每类生成近相同数量图像，保证数据集平衡，输入句子依ImageNet标签，部分模型输入语言有调整扩散模型（diffusion model）： Midjourney、 Wukong、 Stable Diffusio、 ADM、 GLIDE、 VQDM;生成对抗网络（GAN）： BigGAN–&gt;其中SD V1.5最为逼真数据集基准检测假图像检测器假人脸检测器（Fake Face Detector）：专为人脸伪造检测设计，依赖人脸图像的特定特征。代表模型：  F3Net：通过分析频率成分划分和真假人脸频率统计差异进行检测  GramNet：利用全局纹理特征提升检测的鲁棒性和泛化性特点：  训练数据仅为人脸图像，难以直接泛化到非人脸领域  设计思路可启发通用检测器的开发（eg：频率分析、纹理特征）通用假图像检测器（General Fake Image Detector）突破人脸内容的限制，检测各类假图像（如GAN或扩散模型生成）。代表模型：  Spec：以频谱为输入，直接在真实图像中合成GAN伪影，无需依赖特定GAN生成的训练数据 CNNSpot：基于ResNet-50的二分类器，通过特定的预处理、后处理和数据增强优化特点：  现有方法在混合GAN和扩散模型生成图像的数据集上性能不足  急需开发针对此类混合特征的专用检测器检测器能轻松识别同一生成器生成的假图像，说明生成器会留下高度一致的痕迹（如特定频率模式、纹理特征等）。而我们需要提升检测器的泛化能力，即独立于所使用的生成器来区分图像真伪的能力。  →跨生成器图像分类单模型 跨生成器图像分类先在SD V1.4上用七种不同的方法训练的模型 然后用八种不同的检测器进行检验//该表格可反应模型在特定训练数据下的泛化能力，根据各检测器检测准确率平均之前数据可知，Swin-T的泛化能力最强多模型 全组合测试对每个方法 都用8个生成器训练8个模型然后在8个生成器上测试并取平均值//该测试模式反映了方法在所有可能生成器组合下的综合性能退化图像处理图像在传播过程中经常遇到退化问题（eg：低分辨率、压缩和噪声干扰）检测器应该对这些挑战具有鲁棒性→通过评估检测器在这些退化图像上的性能，使之更准确的模拟实际条件  作为baseline model，ResNet-50，DeiT-S和Swin-T都呈现出类似的效果 //数据十分相近  CNNSpot对JPEG压缩和高斯模糊都具有鲁棒性 //因为CNNSpot在训练过程中使用JPEG压缩和高斯模糊作为额外的数据预处理数据预处理即是方法论数据分析真实图像和生成图像的频率分析对比  GAN伪影以规则网格的形式显示来自扩散模型的图像比BigGAN更接近真实的图像    reasons：    在文献Adversarial Perturbations Fool Deepfake Detectors中有提到，上采样方法（上卷积或转置卷积）导致GAN无法正常地近似训练数据的频谱分布，所以GAN生成的图像有较多伪影  因为匹配较低的频率对于所生成的图像的感知质量更重要，而训练期间较少的权重被附加到较高的频率，扩散模型不会在频谱中产生网格状伪影，但是对于较高的频率表现出系统性的不匹配    为验证检测器是否能泛化到不同图像内容类别    数据集：• 训练集：从GenImage的1000类中抽取子集（10、50、100），每类生成固定数量图像• 测试集：覆盖全部1000类，每类50张生成图像，并且来自8种生成器• 真实图像比例：每类真实图像与生成图像数量相同（平衡数据）    控制变量分析可得到，数据集标签的数量对准确度的影响程度远大于数据数量的影响程度  假图像检测器的泛化能力高度依赖训练数据的类别覆盖度，其中100类以上可达到较好效果CONCLUSION：SD V1.4和SD V1.5与Wukong的训练产生了最佳的整体泛化性能GenImage范围广：不仅包含传统的人脸（face）和艺术作品（art）图像，还涵盖更广泛的类别。数据来源：  LFW：用于人脸识别的公开数据集，从中选取了10,000张真实人脸图像，并生成相同数量的合成人脸  Laion-Art：基于Laion-5B的子集，筛选出美学评分高的艺术作品，并从中选取10,000张真实艺术图像，同时生成10,000张合成艺术图像泛化性能优异：  人脸检测：99.9% 准确率（区分LFW真实人脸 vs. SDV1.4生成人脸）  艺术图像检测：95.0% 准确率结论：该数据集在跨内容（人脸、艺术）检测任务上表现出强泛化能力。结论  GenImage是一个专为检测生成模型生成的虚假图像而设计的大规模数据集，其规模、图像内容和生成器多样性均超越以往的数据集和基准。  研究提出了两项任务——跨生成器图像分类和退化图像分类，用于评估现有检测器在GenImage上的性能。  此外，通过对数据集的详细分析，研究揭示了GenImage如何推动开发适用于真实场景的虚假图像检测器。",
      "url": "/posts/GenImage/",
      "date": "2025年04月29日"
    }
    ,
  
    {
      "title": "线性回归模型",
      "categories": "机器学习",
      "content": "//辅助房地产估价 （拟合直线核心概念：  用于训练模型的数据集称为训练集  输入变量（x） ：也称为特征或输入特征//input  输出变量（y） ：输出的量//output target（x，y）即一个训练示例, $x^{(i)}$ , $y^{(i)}$ 表示第i个训练示例训练模型：将训练集提供给学习算法，算法会产生功能function $x \\to f \\to \\hat y$ 该过程即：feature -&gt; model -&gt; prediction $f_{(w,b)}(x)=wx+b$成本函数（也称为代价函数）成本函数的思想是机器学习中最普遍和最重要的思想之一，用于线性回归和训练世界上许多最先进的人工智能模型。👉如何构建成本函数： 平方误差成本函数     \\(J_{(w,b)}= \\frac{1}{2m} \\sum\\limits_{i=1}^m(\\hat{y}^{(i)} -y^{(i)})\\)     \\(J_{(w,b)}= \\frac{1}{2m} \\sum\\limits_{i=1}^m(f_{(w,b)}(x) -y^{(i)})\\)P.S.  m指的是训练示例个数；  将每个预测的y值与真实的y值相差平方求和；  额外除的2是为了后续的计算更加简洁；目标是求： $minimize_{w,b} J(w,b)$  让成本函数尽可能的小，模型的准确度也就越高👉成本函数的可视化若规定了b一定 则图像为二维坐标图像 （汤碗侧切图状）然而用3D图形表示非常不便，于是可以化成等高线地形图的模式去表示梯度下降算法在成本函数图像上，从最高点开始 不断“环顾四周”找到斜率最大的方向并移动一段极小的距离，继续找下降斜率最大的方向，如此重复。\\(w=w-\\alpha \\frac{d}{dw} J_{(w,b)}\\)   \\(b=b-\\alpha \\frac{d}{db} J_{(w,b)}\\)//‘=’作为赋值运算符；$\\alpha$ 被称为学习率；学习率通常是0到1之间的一个小正数$\\alpha$ 所做的是控制下坡的距离（每一步的步长）在曲面图图形中，我们需要采取一些小步子，直到到达值的底部；在梯度下降算法中，我们需要不断重复上述两个公式，直到算法收敛（达到局部最小值）学习率👉如果学习率的值过于大会怎么样？有可能因为步长过长导致越过了成本函数的最小值梯度下降算法实现# 导入库import torchimport numpy as npfrom pyecharts.charts import Linefrom pyecharts.options import TitleOpts, ToolboxOpts# 数据集导入x = np.array([0.18, 0.1, 0.16, 0.08, 0.09, 0.11, 0.17, 0.15, 0.14, 0.13])y = np.array([0.18, 0.1, 0.16, 0.08, 0.09, 0.11, 0.17, 0.15, 0.14, 0.13])# 确定学习率lr = 0.01# 初始化 w，为了减小难度暂时不考虑 b 的赋值w = 10# epoches 为循环进行的次数epoches = 500# 先设置梯度为 0grad = 0# 计算损失函数def loss_new(x, y, w):    return 0.5 * np.sum((w * x - y) ** 2)# 计算梯度def grad_new(x, y, w):    return np.mean((x * w - y) * x)# 核心部分 -- 迭代list_w = []list_loss = []list_grad = []list_i = []for i in range(epoches):    grad = grad_new(x, y, w)    # 更新参数    w = w - lr * grad    loss = loss_new(x, y, w)    print(f\"第{i + 1}次迭代，梯度为{grad}, 权值为{w}, 损失值为{loss}\")    list_w.append(w)    list_i.append(i)    list_loss.append(loss)    list_grad.append(grad)# 绘制梯度与迭代次数的关系图line1 = Line()line1.add_xaxis(list_i)line1.add_yaxis(\"梯度\", list_grad)line1.set_global_opts(    title_opts=TitleOpts(title=\"梯度与迭代次数的关系\", pos_left=\"center\", pos_bottom=\"1%\"),    toolbox_opts=ToolboxOpts(is_show=True),)line1.render()# 绘制损失值与参数的关系图line2 = Line()line2.add_xaxis(list_w)line2.add_yaxis(\"损失值\", list_loss)line2.set_global_opts(    title_opts=TitleOpts(title=\"损失值与参数的关系\", pos_left=\"center\", pos_bottom=\"1%\"),    toolbox_opts=ToolboxOpts(is_show=True),)line2.render()### 运行结果大致情况 ",
      "url": "/posts/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/",
      "date": "2025年04月10日"
    }
    
  
]
