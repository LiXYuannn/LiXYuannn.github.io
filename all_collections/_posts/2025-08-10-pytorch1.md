---
layout: post
title: Pytorch笔记📓第一弹
date: 2025-08-10 00:00:00
categories: ["深度学习","Pytorch"]
---

### Pytorch的核心——张量

在深度学习中，所有的数据和模型参数都用张量表示。

张量（Tensor）可以看成是：

- **0 维**：标量（scalar），比如 3
- **1 维**：向量（vector），比如 [1, 2, 3]
- **2 维**：矩阵（matrix）
- **3 维及以上**：高维数组，比如一批图片 (batch_size, height, width, channels)

📌 在 PyTorch 里：

- **torch.tensor()** 创建张量
- **.shape** 查看维度
- **.dtype** 数据类型
- **.to(device)** 把张量放到 CPU/GPU

### GPU加速

如果有GPU，PyTorch 会自动帮你用 CUDA 加速
```python

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
x = torch.randn(3, 4).to(device)
```

### 自动求导

PyTorch 可以自动计算梯度，这就是深度学习训练的基础。

- 创建张量时加 requires_grad=True，PyTorch 会记录计算图。
- 调用 .backward() 自动反向传播，计算梯度。
- 梯度存放在 .grad 里。

#### 第一个Pytorch文件

```python
#myfirstPytorch
import torch
print(torch.__version__)
  
x=torch.tensor([[1,2],[3,4]],dtype=torch.float32)
y=torch.ones((2,2))

print("x:\n",x)
print("y:\n",y)

z=x+y
print("z:\n",z)

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
x=x.to(device)
y=y.to(device)
x=x.to(device)
print("设备:",device)
  
a=torch.randn(3,4,requires_grad=True)
b=torch.randn(3,4)
c=a*b
out=c.sum()
out.backward()
print("a的梯度:\n",a.grad)

```

```bash
#运行结果
2.6.0
x:
 tensor([[1., 2.],
        [3., 4.]])
y:
 tensor([[1., 1.],
        [1., 1.]])
x:
 tensor([[2., 3.],
        [4., 5.]])
设备: cpu
a的梯度:
 tensor([[ 0.4779,  1.5350,  0.0037,  1.2580],
        [-0.5812,  1.6379,  0.7242,  1.2655],
        [-0.6917, -0.8033, -1.8678, -1.1883]])

```

### 进阶任务 🖊️
- 创建一个 (2, 3, 4) 的随机张量 t（float 类型）    
- 对它做：
    - 加法（+ 1）
    - 减法（- 0.5）
    - 矩阵乘法（用 .matmul() 或 @）*
- 把它移到 GPU（如果没有 GPU 就依然用 CPU），并打印：
    - .shape
    - .dtype

```python

import torch

t=torch.randn(2,3,4,dtype=torch.float32)
x=torch.ones((2,3,4))

print("t:\n",t)
a=t+x
b=t-x*0.5
  
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
# t=t.to(device)
# x=x.to(device)
# a=a.to(device)
# b=b.to(device)
t, x, a, b=[tensor.to(device) for tensor in [t, x, a, b]]
print("设备:\n",device)

print("a:\n",a)
print("b:\n",b)
  
print(".shape:\n",t.shape)
print(".dtype:\n",t.dtype)

```

```bash
#运行结果
t:
 tensor([[[-0.5283,  1.2119,  1.6814,  0.6434],
         [-0.3742, -0.8421, -1.6161,  0.2300],
         [-1.2224,  0.4019, -1.4070,  0.4027]],

        [[ 0.0407, -0.5536, -0.7496,  1.7721],
         [ 1.4842,  0.2181, -0.0732, -1.1741],
         [ 0.4077, -0.7331, -2.2628,  0.8560]]])
设备:
 cpu
a:
 tensor([[[ 0.4717,  2.2119,  2.6814,  1.6434],
         [ 0.6258,  0.1579, -0.6161,  1.2300],
         [-0.2224,  1.4019, -0.4070,  1.4027]],

        [[ 1.0407,  0.4464,  0.2504,  2.7721],
         [ 2.4842,  1.2181,  0.9268, -0.1741],
         [ 1.4077,  0.2669, -1.2628,  1.8560]]])
b:
 tensor([[[-1.0283,  0.7119,  1.1814,  0.1434],
         [-0.8742, -1.3421, -2.1161, -0.2700],
         [-1.7224, -0.0981, -1.9070, -0.0973]],

        [[-0.4593, -1.0536, -1.2496,  1.2721],
         [ 0.9842, -0.2819, -0.5732, -1.6741],
         [-0.0923, -1.2331, -2.7628,  0.3560]]])
.shape:
 torch.Size([2, 3, 4])
.dtype:
 torch.float32
 
```

>矩阵乘法的实现
>——用 .matmul() 或 @

因为张量t相当于2个3* 4的矩阵，不能直接和自己相乘，所以需要再随机生成一个（2，4，k）的矩阵//令k=6

```python

import torch

t=torch.randn(2,3,4,dtype=torch.float32)
s=torch.randn(2,4,6,dtype=torch.float32)
  
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
t,s=[tensor.to(device) for tensor in [t,s]]
print("设备:\n",device)
  
out=torch.matmul(t,s)
#或者out=t@s
print("out:\n",out)
print(".shape:\n",out.shape)
print(".dtype:\n",out.dtype)

```

```bash
#运行结果
设备:
 cpu
out:
 tensor([[[-0.6649, -0.7129,  1.1296, -1.8482,  1.4739,  0.7178],
         [-1.5440,  4.1733, -3.7995, -1.6282, -3.5405, -0.3269],
         [ 2.4297, -4.6300,  0.9426,  2.7147,  1.8590,  4.4622]],

        [[ 1.0501,  0.4767,  1.8069, -3.8165,  2.4047, -1.2312],
         [-1.2006,  0.1548, -0.6207,  1.4950, -1.4686,  1.2318],
         [-1.5264,  0.0297, -0.2412, -1.4789,  1.6743, -1.2765]]])
.shape:
 torch.Size([2, 3, 6])
.dtype:
 torch.float32
 
```

